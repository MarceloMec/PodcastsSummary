{"podcast_details": {"podcast_title": "Jay Shah Podcast", "episode_title": "Risks of AI in real-world and towards Building Robust Security measures | Hyrum Anderson", "episode_image": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/38361858/cc657ce8428915a7.jpeg", "episode_transcript": " One other attack is called prompt stealing. And it's that, can I... So the system prompt is not known to you as a user. When you go to chat GPT, there is a system prompt, but we don't know it. So you only get your user prompt. But one attack is, can I give a clever set of instructions as a user so that I can kind of retrieve from working memory what this meta prompt is? And if I have it, it opens up a world of possibility for me to manipulate the model, to misuse it in a number of ways. All right. So hello and welcome everyone to who's ever listening to this particular podcast. Today I have with me Dr. Hiram Anderson. He's a distinguished machine learning engineer at Robust Intelligence. Prior to that, he was a principal architect of trustworthy machine learning at Microsoft, where he also founded Microsoft's AI Red team. He also led security research at MIT Lincoln Laboratories, Sandia National Laboratories, Mendiant, and also was chief scientist at Endgame, which was later acquired by Elastic. He's also the co-author of book, Not a Buck, but with a Sticker. And his research interests include assessing the security and privacy of machine learning systems and building robust AI models, which we'll talk more about in this podcast. It's nice to have you, Dr. Anderson. Welcome to the podcast. Thank you, Jay, for having me. So for us to know, what was your entry point in AI? Like how did you get started into AI? And then how did you go on to researching more about security and privacy in machine learning? Well, my entry point to AI actually happened as an undergrad, believe it or not, way back in the nineties, AI was a thing. It was maybe different than it is now, but all the foundations for what we see today existed. So I did my undergrad and my master's degree at Brigham Young University and had some fantastic instructors, both in the computer science department, but also in my home department of electrical engineering, we had fundamental math courses called detection estimation theory, which to me provided an amazing foundation. So I later went on and did a PhD at the University of Washington in signal processing and machine learning. We had another just phenomenal advisor, Maya Gupta, who introduced me to the world that we now at least call AI. Yeah, interesting. And how did you get into understanding what are the security and privacy concerns of machine learning? Great question. Well, that hasn't always been the case. I've always been using machine learning for, let's say, security with a big, big S concerns, including national security in the national laboratories. And it kind of dovetailed into my interest in cybersecurity. And I joined this company called Mandiant, which I think the same year that I joined had just published a report called the APT-1 report about the security of machine learning. APT-1 report about alleged Chinese interference into US corporate interests and named the unit and such. So I was very interested in this company wanted to take a chance on what does machine learning look like? What does data science look like for cybersecurity? So the great thing about cybersecurity from an interest standpoint is that there are, by definition, there are actors actively trying to subvert whatever tool that you produce. And when that tool is based on machine learning, there are very clever things that attackers will do to try to get around that. And around that same time, all of the academic interest in adversarial machine learning, which I'll define as systematically probing worst case failure modes of machine learning models, not necessarily with an attacker in mind, but kind of like presenting the worst case, bounding the bad things that could happen. But those all kind of happen at the same time. And timing is everything, I think, for me. And so that's how I got ushered into this field. Securing AI. Yeah. And just to lay down the landscape, so like most of the people listening to this podcast might already know what kind of attacks we mean by when we say internet security or cyber security, because people are trying to get into the system. How do these differ when we say an ML model is in place? So let's say if you have a pipeline or software that uses ML or AI model at the back end, what do these attacks look like that are much more different than what we see in the attacks look like that are much more different than what we have known through cyber security or internet security? It's a great question. And the first thing to remember actually is what they have in common, and that machine learning systems are software systems. So guess what? You inherit those vulnerabilities. So those vulnerabilities are the vulnerabilities in code, in the platforms that they're served on, in the APIs that you use to access a machine learning model, a model that's serving kind of results. But in addition to that, there's this whole new class of vulnerabilities. And they're actually properties of the model, properties of all models that are inherited because of the data set and the fact that models are a summary of that data set, an imperfect summary. And the imperfectness of that all can be exploited in many, many ways. So how is it different than regular? So first, that the classes of these vulnerabilities include confidentiality vulnerabilities, that data or approximations of data can be stolen by me interrogating a model. That should blow your mind. So without access to the data, yet an attacker can approximate the data that went into that model or approximate properties of the data, attributes of the data set that underlie the model training. They also include the integrity of the model. So can I present inputs that would cause the model to perform in a way that I want as an attacker? Example of this would be a malware classifier. I would like it to not recognize my malware as being malicious but benign. So I can choose very carefully how to manipulate the malware that I release so that commercial anti-virus applications based on machine learning do not recognize it. And finally, there are even attacks to availability. So just like there's DDoS attacks against network systems, they're actually really clever attacks against machine learning models where certain inputs maximize the time or energy consumed. So at the very least, it takes more time to compute or costs more for the vendor to supply you an answer. And those are analogous to DDoS. The difference, though, is that these are not models where I can isolate code and fix it. These are bugs that can't be patched in a sense. And it makes them much more difficult to work with. And we can talk later about how do you deal with them. But it makes management of this a really interesting space and one that many even really mature organizations are still working to figure out. Yeah. And one thing you did mention about the example that people can draw inferences based on what kind of data it has been trained on by doing a lot of inferences and getting an idea of like understanding the summary of the data that the model has learned. And what are the frameworks that do you think that these models can be exploited at? So this normally happens at the inference stage where you can say the model has been trained and it is deployed in a real world scenario. People do like a very inference based like lots of inferences and I can get a summary. What other parts of the framework you think can be exploited by these attacks of such an end-to-end deep learning model? A really great question. I think today, in fact, a lot of the concern because of the popularity of open source is actually upstream from inference and it's in the supply chain. Just five or six years ago, the standard way to build a model would be I collect my data set, I curate my data set, I train my model, I test my model. You know, it's all kind of in-house. Modern machine learning pipelines look like this. I download a model, I fine tune a model using sometimes my but sometimes public data. And so the kinds of things now that we're worried about in security are this supply chain. The supply chain consists of the data that your model relies on, the model weights themselves, and it also includes the software including how those model weights are delivered. There's famous examples of showing how, for example, a PyTorch file is actually just to realize on the pickle Python format which can be instrumented to execute arbitrary code. So data, of course, if I can affect your data, I can inject data and make the model now in a causative way before you release it to behave in a way that will be useful for me later as an attacker. Interesting. And one thing you did mention, most of the organizations are still learning how to patch these bugs. Are there any kind of operational risks that these kind of companies incur? Because I would assume these AI models are being deployed by these organizations on a daily basis. Let's say for Google, they're using AI at the back end for a trillion searches a day or I don't know what other statistics. What are the operational risks that normally go into that limit these people to deploy these security measures? Security measures in real world scenarios? Yeah. The interesting thing about real world is that there's actually a bit of a gap between what is being studied in academic settings and what happens, what attackers actually do in the wild. And for those interested, there's a great paper that my colleagues and I have produced, actually called Real Attackers Don't Compute Gradients and talks about these differences. And basically, what it comes down to is that attackers, like in academia, we often do these mathematically rigorous optimization processes to cause these effects in the models. But when you look at what's happening in the wild, attackers are just clever. They use intuition. And so what companies, back to your regional question, what companies you brought up, Google, many other mature companies, they've also just been clever. So sometimes they're very easy, non-sophisticated approaches that can protect your model. Let me give you an example. Content moderation. So if an attacker in this sense is somebody who wants to upload illicit content or copyrighted content to, I don't know, YouTube or LinkedIn or whatever, and content moderation filters will catch them. And an attacker, what an attacker does is an attacker will try to evade these content moderation filters until she or he is successful uploading the malicious or inappropriate or whatever content. Well, OK, so what does the attacker do? The attacker tries stuff until it works. That's the sophistication today. High impact, low sophistication. What the defenders are doing today is also relatively effective, but not very sophisticated. Their policy is like, if I catch you trying something three times, I'm going to freeze your account. And it's very easy to do. And it doesn't prevent an attacker from succeeding, a persistent attacker, but it does raise the costs. It becomes frustrating for the attacker. And those kinds of deterrents tend to work pretty well today. They may not always work. And that's why this is an evolving field. But that's kind of what has worked in traditional machine learning and security. Yeah. But to get your overview of what could be a potential solution, I'm driving a point of like, because I would assume that these kind of attackers would have like infinite amount of variabilities and there would always be something to exploit. So how would you say, like, what is the end point? So for example, in the example that you gave, content moderation, right? So like, it's very hard to define like a very strict policy, what counts for illicit content versus not, right? So like a human or any kind of programmer won't be able to decide all the scenarios that can count for illicit content. And I would say that that was a reason like they would limit certain kind of behavior so that to flag if this is like a bad or good base. So how would you suggest, like, do you think that this needs to happen from an algorithmic standpoint that we need to define or design models that are much more capable of understanding what's bad versus good? Or do you think this comes down to policy of like, how do I design like three attempts and you're banned, five attempts and you're banned? All of these are applicable. And in fact, in security, we often talk about defense and depth. And often people like me who are engineers, we think about defense and depth in being technological, overlapping technological solutions. We're like, just do the math. If you're successful 90% of the time, or see if an attacker is successful 1% of the time at each layer, but you have 100 layers, 1% to the 100th power, you essentially these diminishing odds of an attacker succeeding as you layer these. But policy also fits into this defense and depth idea. And it includes policies like three strikes, you're out, but it also includes more generic policies about how as a provider of systems, the kinds of security measures that I take, the kinds of the kinds of information I reveal for my mission in your service, right? Like example, I don't always give you a score to eight significant digits. I only give you a hard label, which makes it more annoying to an attacker. Things like that, right? Yeah. And you talk about policies. I always wanted to learn more about how do these policies play a role into regulation of AI systems? And you might have a better experience working with these. Like what is really hard about defining these regulatory standards when it comes to AI models in place? Because is it because that the AI models are black box, the nature of like how AI models operate? Is that the reason or is it something on like how confidence these models are? And that is being unsure to academics, researchers, and industry professionals. Like what is the missing block that makes it at least at this point really hard to define regulatory standards for AI systems? I would say that the gap between desired policy outcomes and technical feasibility, that's one of the issues. So an example is we want all models to be explainable and secure and not biased in the sense of performing safe and fair outcomes no matter who is the attacker. In the sense of performing safe and fair outcomes no matter who is acquiring the model. Oftentimes, those notions can be at odds with each other. So it's one thing to be able to set a policy, but it's actually hard to live up to the policy in all of those facets. Another issue is the fact that when there's a stated policy that like say a regulatory regulation would enforce, the way that it is enforced might also not actually solve the policy. It feels in some sense like a cargo cult. We're doing our best to apply this. We have all the signs of solving the problem, but the problem is not actually solved. An example of this is explainability. Famously, Himalaya Raju at the Harvard Business School demonstrated how explainable models that tell you why they have made a prediction are also extremely good at convincing you they've made a fair decision when in fact they haven't. So we've satisfied the explainability, but actually made the bias and fairness problem worse. This is just scratching the surface about why policy is hard. What I really like though is that this doesn't necessarily all have to come from government regulatory compliance. There are companies who are taking upon themselves not telling others to do it, but just interested in their own risk mitigation processes to set internal policies, to adopt standards that are being proposed and are being revised in the open forum. That is so helpful. In fact, my own opinion is those would lead any kind of robust regulatory compliance so that we can learn through this process of trying out internal policy that affects just me and my customers. Once you get enough of these industry verticals, you can have a more overarching policy that maybe doesn't make sense. Yeah. But do you think that there could be a global regulatory standard for AI that can be put in place? Because I would assume, and this is just from my naive understanding, that AI models are being used in different fields very differently. Right? Like in healthcare, there are certain things that AI models can be really good at versus it can be exploited at. But it's completely different when it comes to, let's say, content moderation for someone like social media platforms versus something like Google, which has a high impact, something at Microsoft. Would you say that there could be a common thread that can be defined as a regulatory standard for AI models? Really great observation. And I do believe there are common themes, but the themes are so generic that you wouldn't maybe recognize in the industry. A good example is NIST's AI risk management framework, which very generically and purposefully generically outlines how to create a culture for risk management for companies who do AI, how to map and measure and manage that risk. Right? And it doesn't say anything about specific requirements, but what it does that's really actually more important is it tries to help organizations themselves adopt a mindset and a culture where they can determine how to apply those kinds of concepts. So from this, yes, there are regulations, regulatory compliance about employment in the New York City hiring law. There's some that are going to affect exclusively financial services and being fair to the loans that are given using AI, et cetera. That will happen in these industry vertical, but the common theme are these broader principles that might at the outset look devoid of specific recommendations, but actually are really, really important. And I would encourage those who are listening to go take a look and try to see yourself in something like NIST's AI risk management framework. They've released a number of playbooks as well to help you see yourself and how you'd adopt this in your own company, but they're very important and shouldn't be ignored. Yeah. And I think... Yeah, go ahead. Oh, I was going to say, I'll just point out they are not enforced. They are not intended to be enforced. They are a tool for you to choose to adopt yourself. Yeah. Yeah, and I think I feel like this is just maybe me seeing these things from a very viewer perspective is the margin for error has become very small for AI-based systems to be in place. Something goes out and people just start attacking it very, very harshly versus... Because these AI models, I feel, are still being deployed and being tested. It's like a completely new field for researchers or security practitioners to really understand where it fails. Normally, when we have engineering systems, we have lots of beta testing to see where it can fail and where people can misuse. But versus for AI, it remains a very unknown thing versus people are rushing to implement and deploy this in the real world. So it becomes a very clash between people want to push it to production versus we don't know where exactly it fails. So it's like constantly... We are trying to understand on the fly how these things are interacting with the world. Yeah, I totally agree. This pattern is not without precedent. It's really good that we had early attackers who even did it for fun, hackers, I should say, when the internet was coming to be in mass, the dot-com era, where security and online services co-developed and it made it in the end a stronger, safer place. I think we're seeing that today also with AI. If we're honest with ourselves, most of the advancements with AI today, most of the big changes, the innovation, there's obviously lots happening still in research and academia, but I would say most of it is actually in the application space. It's plugging things, wiring things together. That's also where some of the biggest security needs are. And so this co-evolution of penetration testing of these new paradigms with the novel applications of things, I think is a little bit like the beginning of the internet, where we're going to co-evolve and make it stronger. And we're going to see bad things happen along the way, but ultimately, you won't stop using the internet. You'll continue using it. Yeah, but talking about something that you mentioned, we are trying to solve this and co-evolving, but if you see many of the advancements in AI, let's say generative AI, so we have seen these images of Pope, we have seen deepfakes, which are problematic. Most of us humans, people who might not have any AI background, they might just feel this is correct and it can cause many lots of potential harms like impersonation and all those kind of tools. And one of the ways, if you see the narrative, one area says that, okay, we should just ban this kind of research because this can create and it can just go out of hands of how this can be misused. And the other idea, what you say, it is co-evolving. We are trying to understand how this can be mitigated. So I want to know, first of all, your perspective, what is the immediate step that needs to be taken when we talk about generative AI? Because yet we don't have any standards or measures to see the misuse or, like I would say, track the misuse. And I would like to ask, what are the potential steps any kind of researcher or security practitioner should keep in mind when it comes to generative AI? So maybe a lot to unpack, but underneath all of this is this healthy realization that bad things can happen with generative AI. Not least among them, maybe the most among them is misinformation and factual incorrectness, emitting things that are copyrighted. There's a lot of risk there. So the first thing is that as developers of this technology, you can just be aware that that's possible. But very concretely, though, when it comes to securing AI, there's just a few simple things that if you design with this in mind, it will save you a lot of pain. And one of those is that we don't treat generative models as trusted users. And what I mean by that is that often it's very tempting, and we do this all the time with common frameworks as we chain the output of large language model to a plugin or an app that does something. And if that something is sensitive, we need to be extra careful. You have to just ask yourself if you let this large language model inside a trust boundary where you wouldn't want an untrusted user. You need to treat the large language model like an untrusted user, sanitizing and filtering its output, understanding what access and what roles it should take in your application. If you're strict about that, then you can save yourself a lot of headache down the line of risk that will happen. Yeah. And correct me if I'm wrong. I think these kind of policy or any kind of security measures that are being taken in place also depends on the context of these applications being deployed. That would be security measures completely different when it comes to, let's say, video based content moderation versus let's say text based content moderation. And when it comes to that's a healthcare, is it the case that normally we need to have different set of policies, different set of principles when it comes to security and machine learning AI models? Yeah. Your policies will depend on your application. There's going to be a lot of overlap and common things. So like this idea of trust boundary is like a relatively high level notion that will be implemented differently depending on your specific application. Yeah. Yeah. And one thing you earlier mentioned when we started this conversation is something that you said, attackers can also manipulate the trained model. Because the one example that we took was the inference where I can understand what is the abstraction of data, but you also said I can potentially influence the model. Can you explain to us how would that happen? Because this is like also curiosity on my end, because when I say, let's say there is a model deployed and it is doing inference on any kind of data, how do I change the model weights, let's say, and how do I influence that particular model? So for a model that's already been deployed, it turns out that you don't need to change the model weights in order to make it, in order to force to behave in ways that you as an attacker would want it to behave. And the way to think about, one useful way to think about that is that in all real world practical problems, models are imperfect. They're going to make mispredictions, they're going to generate slightly the wrong text, a small fraction of the time. Right? And what an attacker, like what a sophisticated attacker, an adversarial machine learning research can do is exploit that fact. And what you realize happens is that the kinds of failures that have, those kinds of failures can be triggered. And I can discover the triggering mechanism to make that happen by interactively querying, interrogating the model. And this is an iterative process where I observe the output, and then I tweak the input in a certain way so that I come ever closer to my objective of, say, getting the model to generate inappropriate racist content. Right? I can force it to do that. I can coax it, I can lead it in that direction by iteratively interrogating the model. And during this process, there's been no change in the model weights, the model system meta prompt or anything. It's just me exploiting the existing failures that are inherent in the model anyway. Yeah. Yeah. Okay. Okay. That's fair. And in terms of, let's say, because a lot of people won't have the background of working with AI models or something like, let's say, chargeability, because that's something being used by everyone and everyday nowadays. So would you say that I can prompt these models enough to understand what kind of data it has been trained on versus that it has been hidden from it? Like, would that be a valid adversary at that time? That would be. And you'd break it down. Like, one of the great security challenges in kind of like chat GPT sort of language models today is called prompt stealing. And to describe this attack, you first have to understand that the model itself is only half the battle. The other half of the bottle is what's called the system prompt or meta prompt. And it is all the human intuition and instructions given in plain English to the model about how to answer the user's question in the right time, in the right way, and what's so called properly aligned to a polite, a human you want. The social norm. Yeah, exactly. So this whole concept of jailbreaking is getting outside of that alignment. It's causing it. One other attack is called prompt stealing. And it's that can I... So the system prompt is not known to you as a user. When you go to chat GPT, there is a system prompt, but we don't know it. So you only get your user prompt. But one attack is can I give a clever set of instructions as a user so that I can kind of retrieve from working memory what this meta prompt is? And if I have it, it opens up a world of possibility for me to manipulate the model to misuse it in a number of ways. So that's an example of a confidentiality attack, stealing data at inference time. In this case, the model that data is not baked into the model, but it's like a sidecar to the model. It's really, really important to how the model functions. Yeah. Go ahead. Go ahead. Oh, there's of course more. You know, one of the great jobs of any red team is to... Any development team is to make sure that there's no, you know, let's call it bad data that your model gets fined tuned on. What's bad data? It's copyrighted information. It's personally identifiable information. It's offensive information. You don't want that to get in the model. And great efforts go into scrubbing these data before they are used in training or fine tuning. Unfortunately, those scrubbing methods often fail. And there are ways now to... You can't find them. They're just too big to manually comb through. So you can only resort to these, you know, think of like doing regex queries for every possible way to surface this bad data. But then in red teaming, often what one will do, a team will do is come in and come in independently without access to the data, but query the model and try to tease out. So you thought of social security number, you thought of personally identified... What about a credit card number? Can I get you to spend a credit card number? And in fact, if you're persistent enough and the data exists in the training set, you can tease these things out. And that's why red teaming is so important by independent party to kind of check the work of the teams putting these protections in place. I see. And can you elaborate a little bit more about, I think, AI Red Team that you guys co-founded at Microsoft, what was the intuition behind it? Like what kind of events or what kind of experiences that you guys saw working with these kinds of models that led you to do that? And what's the overall overarching definition of that particular team? So let me just give a shout out to my book co-author, Ram Shankarasevi Kumar, who is leading those efforts at Microsoft. I'm no longer at Microsoft. I do know that the work they do continues to be so, so, so important, not just for Microsoft, but really setting a standard for our industry. Second, I'll say, I won't come on any particular engagement, obviously, we have made one of them public, one of the findings that are really interesting. But let me just tell you, like during the time I was there, at least a couple of things to note, we were never unsuccessful in our red team exercise, never. And this is not surprising. Red teams usually are successful, but in AI Red Teaming, we always found something. Right. The second thing to remember is that the way we framed AI Red Teaming was a little different than some cybersecurity notions of red teaming in which we like to partner with the team and make it a code development or a co-evolution. So they would say, we would ask them, like, what's your worst nightmare? And let's try to make that happen. And that's what we did. So we would try to get them to say, okay, let's try to make that happen. And that's kind of how Red Team would start, as opposed to sneaking around, surprise attacking a product without them knowing about it. At the end of the day, we want to improve security, we want to work with the team, what we're not, we're not going to release a report to surprise them. We want to give them information that will help them. So those are the foundations for the red team. So one example that's public that you can read about, you can see information about this in various formats, including an Enigma talk, I think in 2021 that I did, or maybe 2020. But this was a internal model. So here's what's interesting. Internal model, meaning the attacker did not have direct access to query the model. It was the model was inside of some system that my query would percolate into, have some interaction with the model and the model would affect something about the system to come out. So it was, I didn't have direct access, isolated access to it. And it was in an application that you wouldn't even, think that machine learning was important. This is for managing compute resources. So the model's job was to efficiently suggest how to manage those resources to save money for the company. And what we found is that a couple of lessons learned. Number one is that just because your model is internal, it's not on the internet, does not make it safe to address some manipulation because there was a path, an indirect path from the internet to that model, which we can find. Second, the way we did it, we found a lot about, we took on the persona of an external entity, we found out about the model from publicly disclosed information. That's how we knew it was there. Not by, not actually by, that the team knew we were doing this, but our rules of engagement were, we'll go find out about this on our own. And I guess the lesson here is that basically, if you think that the model you're developing is protected because it's hidden, it's harder for sure, but it's not bulletproof. And if your model is using a really important application in finance or some other highly regulated industry, there are ways to have this affected by an attacker on the outside. The saying in cybersecurity is there's no security by obscurity. I would modify that. I definitely think there is some security by obscurity, but it's not bulletproof. And so still adopting just regular old security best practices can help you a lot in those situations. Yeah, I mean, from what I hear, it always feels like a constantly moving target, right? So there's not an ideal milestone that we can reach in any kind of these software systems, whether it is using AI models or not, because once you develop a security practice, like hackers or adversary attackers would at least find some of the other way to attack using some kind of patterns that they find using these attack mechanisms. So I would like, is it fair to say that there won't be like a very ideal case milestone to be reached where we can say, oh, it is safe for people to use, but it will always be like an assumption to understand that yeah, these models can be exploited. We should just be aware and keep our efforts rolled in these kind of domains. Yes, a great way to say that. Maybe state another way. It's all about increasing costs to an attacker, reducing risk for yourself, managing risk in a process. So we should think about these in terms of ongoing things. On the AI Red team, we never wanted to be like a rubber stamp of approval saying you're safe. No, our job was to raise findings for you to manage risk and begin an ongoing process of risk management. So it's very much not a binary thing as you're saying. And one question I had outside of this was we have these kind of regulations. Let's say European Union has a very strict set of regulations when it comes to AI models or in general, any technological solutions. Versus I think a lot of other countries don't really have that much of a strong regulatory board. And correct me if I'm wrong. I think even US has limited amount of regulations for AI models. Are there any, like is it because it is so hard to define like we discussed earlier or are there any other factors that lead to not having like a very strong regulatory set of rules as we have also let's say GDPR or something that came out a few years back? It's an interesting question. And I would say it is difficult, but we're not without guidance in this. I think it's more to do, like if you're comparing United States and Europe, I think it has much more to do with kind of a cultural development process. So an example is, and this we also bring up in this book that Ram and I have written is there's a way for products to be certified as safe. So example, your toaster, if you turn it upside down, it's gonna have, if you're in the United States, it's gonna have a symbol on it that says UL, underwriting laboratory. It means it's been tested so that it, you know, does not spontaneously combust into flames when you pop down your toaster, right? And what's interesting about UL is that it is not required. It is, at least it's not required by the government. It might be advantageous to a product to adopt that so they can say they have the seal. In Europe, there's a kind of equivalent notion still called CE and CE is required by the government and you need to have it for these applications. And so you see a similar kind of culture happening in AI regulation. The EU AI Act is, you can think about it as like an evolution of GDPR now for models. And it's now the government's mandating how you should do things top down. I don't wanna call the US behind. In fact, we have great efforts. I mentioned NIS AI RMF, there are many, many others. There are standards being developed and worked on, but it's very much more of this grassroots, let's figure it out, let's adopt it. Maybe it's like capitalism in effect or something. So I think that's what you're seeing. What will happen though is just like when GDPR was passed, it caused US companies and US consumers to also follow suit. We began acknowledging that we're accepting cookies in our websites, for example, even though it was not required in the state of Arizona to do so. But it's because of this so-called Brussels effect. What happens in Brussels will flutter over here and have an effect also. We're gonna see the same thing in the EU AI Act, even as there are other things, regulatory and standards and otherwise in the United States. We're all gonna be affecting each other here. Interesting, yeah, that's good to know. That's good to know. And before we sign off, I wanna spend also some time because yeah, I mean, first of all, thanks to you and Ramm for sending me the copy of the book, not a bug, but with a sticker. So I think I got a chance to read the whole book. Fantastic, there are lots of things like being a researcher. I did not have the exposure to learning more about where these models can be failing at because I completely work in a different field, but it was very interesting. But I wanna let you maybe explain what can readers expect to learn more about it? Because I mean, even though I think a lot of topics we covered in this podcast are a part of that book, so that would be like a trailer for them, but can you cover more topics? What was the intention for you and Ramm to consider writing this book? What was the intention that you guys wrote this book and what do you think readers should know about ML and AI models? Thank you for the question. And let me just say first, I wanna give credit, Ramm Shankar Sivakumar is just a mastermind and this really is, you know, at least 90% is his passion and his work and his brainchild. And I was so grateful for a wonderful partner in this journey. And what we talked about when formulating this book, like who are we writing this for? So first note, this is not a textbook. It is not, you know, it is not technical in that way. This is actually meant for decision makers. There's more storytelling than proofs, right? So what you'll find as you read this book is first, we interviewed over a hundred experts and business leaders and policymakers and, you know, leaders in AI, leaders in security about these issues. And so one really fun thing about this book is their stories that you're gonna hear about. Even for people who have been in adversarial machine learning or security for a long time, you will find things in here that are delightful even backstories, even if you've memorized the textbooks on these. For those who are kind of dipping their toe in this water of security and machine learning, you're gonna find a gentle introduction to all of the things that can go wrong, beginning with the easy stuff. I mentioned content moderation and trial and error. Actually, like that's the bulk of what we're talking about. Actually, like that's the bulk of stuff we see in the wild today. We also spend a fair amount of time on the more sophisticated algorithmic style attacks that I mentioned of confidentiality, integrity and availability. We also touch on the policies that you Jay have been, we've been talking about today, how those are being shaped. What can be done from a technological perspective versus a policy perspective. So I think when you're done reading this book, what I hope every reader will come away with is number one, if you're building a machine learning model, you are gonna be asking yourself the right questions about security. If you're buying an AI technology, you're gonna have a critical eye and you're gonna be wise to how it's used. But at the end of the day, we are very pro AI. We're not, we think it's gonna be a great thing for the world as we talked about, is we're co-evolving with a security mindset. Yeah. In addition to that, like what I found cool about that book, I think you guys are pledging the charity, like this whole profit that you are getting from the books to a charity. Can you tell more about that charity and why did you choose that one? Yes, actually. So Ram and I have chosen two charities for which all of our author proceeds, royalties and things from the book will be donated to. And just wanna preface this, this has a theme. And the theme here is that this conversation about AI and security and risk, it affects everybody, right? And it shouldn't be solved by just somebody who has a PhD in machine learning or just a policymaker. It needs a lot of voices. And so our choice of charities has to do with bringing more people into the field. And the first charity we picked is called Black in AI. And it is a charity focused on serving underrepresented members of our community to give them a voice in AI. A surprisingly few number of PhD graduates from computer science identified as black. And that has a lot to do with the opportunities given them to succeed. So this is a charity that promotes and gives a healthy community for black members of the community to make a contribution. The second charity is called Bountiful Children. And number one, it's a 0% overhead. It's a volunteer organization that focuses on child malnutrition. And what's really interesting to me about this is that the number one gate between being born and getting a good education is actually nutrition. It's children having enough nutrition to go to school and think and focus in school. So this is exactly solving that problem. And we'd love for you to go to these websites and figure out a way that you can contribute also to Black in AI, to Bountiful Children's organization. And if you buy the book, then 50% of the proceeds will go to Black in AI and 50% will go to Bountiful Children's. Yeah, I mean, kudos to you guys. I think that's a wonderful initiative for sure. And for anyone who's like planning to buy the book, I think one of the few things that I personally liked apart from what the insights that you guys have shared is, we live in a world where a lot of people are pushing towards understanding that AI is gonna automate a lot of jobs that are existential threats. But at the other hand, when we look about these kinds of threats and loopholes into the AI systems, we really see AI is not much more of a magical instrument. It is just a bunch of lots of cool mathematical tricks that we put into a model. So there are lots and lots of bugs. And once you realize that there are so many bugs, you get the idea a little bit away from that, yeah, AI is gonna automate a lot of jobs because it is nowhere to being perfect. So I think one of the key things I also shared with one of my lab mates, this particular book, and she had the same insight that, yes, we are far from anyone claiming that AI is gonna take over humanity or take over any kind of existential threats, only if they understand they are failing at so many levels that we need to fix, first of all, in order to make it really concrete. So that's just something that at least I took at this particular time point in my life. So thanks for that. Like it's a wonderful book for sure. But apart from that, yeah, I think we are close to our time slot. So I think we got a chance to cover a lot of particular topics that I personally had the curiosity even after reading the book. So thanks for covering those topics. We covered, I think, lots of failure points that at least point us to what people can learn more about by revisiting your and Ram's profile. What are these kinds of entry points of where the machine learning failure points could be. And I'll also leave link to whatever we discussed, a lot of things that we discussed that could have like a pointer and also leave a link to yours and Ram's profile. So that, like I think I learned a lot more about reading about your articles, your and Ram's. So I think that it will be useful for researchers. But apart from that, thanks a lot for being here. It was really insightful. And hopefully people find interesting things from your profile and maybe they are motivated to learn more. So yeah. Thank you, Jay. And thanks to your listeners. Thank you."}, "podcast_summary": "In this podcast episode, Dr. Hiram Anderson discusses the security and privacy concerns of machine learning systems. He explains various attacks, such as prompt stealing, where an attacker tries to retrieve the system prompt of a language model to manipulate its behavior. Dr. Anderson emphasizes the importance of addressing vulnerabilities in machine learning models, as well as the need to increase costs for attackers and reduce risks for organizations. He also highlights the ongoing co-evolution of AI and security, and the importance of adopting a security mindset when developing and using AI models. Additionally, Dr. Anderson mentions the book \"Not a Bug, but a Feature,\" which he co-authored with Ram Shankaraseva Kumar. The book provides insights into the risks and challenges associated with deploying AI and offers practical advice for decision makers. The authors are donating all their proceeds from the book to charities Black in AI and Bountiful Children.", "podcast_guest": "Dr. Hiram Anderson", "podcast_highlights": "You're welcome. I'm glad I could assist you. If you have any more questions, feel free to ask."}